{
  "best_metric": 0.6920289855072463,
  "best_model_checkpoint": "/content/drive/MyDrive/Colab Notebooks/NLP/NLP_Project/Model/lora_finetuned_model_2/checkpoint-18575",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 18575,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013458950201884253,
      "grad_norm": 2.9943933486938477,
      "learning_rate": 4.986541049798116e-05,
      "loss": 1.2794,
      "step": 50
    },
    {
      "epoch": 0.026917900403768506,
      "grad_norm": 2.610163927078247,
      "learning_rate": 4.973082099596232e-05,
      "loss": 1.1334,
      "step": 100
    },
    {
      "epoch": 0.040376850605652756,
      "grad_norm": 3.9431607723236084,
      "learning_rate": 4.959623149394347e-05,
      "loss": 1.1373,
      "step": 150
    },
    {
      "epoch": 0.05383580080753701,
      "grad_norm": 7.171976566314697,
      "learning_rate": 4.9461641991924634e-05,
      "loss": 1.0981,
      "step": 200
    },
    {
      "epoch": 0.06729475100942127,
      "grad_norm": 2.956336736679077,
      "learning_rate": 4.932705248990579e-05,
      "loss": 1.104,
      "step": 250
    },
    {
      "epoch": 0.08075370121130551,
      "grad_norm": 4.277462959289551,
      "learning_rate": 4.919246298788695e-05,
      "loss": 1.1235,
      "step": 300
    },
    {
      "epoch": 0.09421265141318977,
      "grad_norm": 5.977795600891113,
      "learning_rate": 4.90578734858681e-05,
      "loss": 1.0738,
      "step": 350
    },
    {
      "epoch": 0.10767160161507403,
      "grad_norm": 3.05161714553833,
      "learning_rate": 4.892328398384926e-05,
      "loss": 1.0394,
      "step": 400
    },
    {
      "epoch": 0.12113055181695828,
      "grad_norm": 6.06802225112915,
      "learning_rate": 4.878869448183042e-05,
      "loss": 1.0728,
      "step": 450
    },
    {
      "epoch": 0.13458950201884254,
      "grad_norm": 3.3788490295410156,
      "learning_rate": 4.865410497981158e-05,
      "loss": 1.0389,
      "step": 500
    },
    {
      "epoch": 0.1480484522207268,
      "grad_norm": 3.4586455821990967,
      "learning_rate": 4.8519515477792735e-05,
      "loss": 0.9793,
      "step": 550
    },
    {
      "epoch": 0.16150740242261102,
      "grad_norm": 3.5084354877471924,
      "learning_rate": 4.838492597577389e-05,
      "loss": 0.9571,
      "step": 600
    },
    {
      "epoch": 0.17496635262449528,
      "grad_norm": 3.541299819946289,
      "learning_rate": 4.825033647375505e-05,
      "loss": 1.0068,
      "step": 650
    },
    {
      "epoch": 0.18842530282637954,
      "grad_norm": 4.110055446624756,
      "learning_rate": 4.811574697173621e-05,
      "loss": 0.9431,
      "step": 700
    },
    {
      "epoch": 0.2018842530282638,
      "grad_norm": 6.967580318450928,
      "learning_rate": 4.798115746971737e-05,
      "loss": 0.9394,
      "step": 750
    },
    {
      "epoch": 0.21534320323014805,
      "grad_norm": 5.112771987915039,
      "learning_rate": 4.784656796769852e-05,
      "loss": 0.871,
      "step": 800
    },
    {
      "epoch": 0.2288021534320323,
      "grad_norm": 6.426032543182373,
      "learning_rate": 4.771197846567968e-05,
      "loss": 1.003,
      "step": 850
    },
    {
      "epoch": 0.24226110363391656,
      "grad_norm": 6.152453422546387,
      "learning_rate": 4.7577388963660836e-05,
      "loss": 0.9151,
      "step": 900
    },
    {
      "epoch": 0.2557200538358008,
      "grad_norm": 5.042302131652832,
      "learning_rate": 4.7442799461642e-05,
      "loss": 0.9313,
      "step": 950
    },
    {
      "epoch": 0.2691790040376851,
      "grad_norm": 2.844297170639038,
      "learning_rate": 4.730820995962315e-05,
      "loss": 0.8698,
      "step": 1000
    },
    {
      "epoch": 0.28263795423956933,
      "grad_norm": 5.035127639770508,
      "learning_rate": 4.717362045760431e-05,
      "loss": 0.931,
      "step": 1050
    },
    {
      "epoch": 0.2960969044414536,
      "grad_norm": 10.874808311462402,
      "learning_rate": 4.703903095558547e-05,
      "loss": 0.8026,
      "step": 1100
    },
    {
      "epoch": 0.30955585464333785,
      "grad_norm": 6.242783546447754,
      "learning_rate": 4.6904441453566624e-05,
      "loss": 0.8582,
      "step": 1150
    },
    {
      "epoch": 0.32301480484522205,
      "grad_norm": 6.809597492218018,
      "learning_rate": 4.676985195154778e-05,
      "loss": 0.9139,
      "step": 1200
    },
    {
      "epoch": 0.3364737550471063,
      "grad_norm": 5.833324432373047,
      "learning_rate": 4.663526244952894e-05,
      "loss": 0.9132,
      "step": 1250
    },
    {
      "epoch": 0.34993270524899056,
      "grad_norm": 7.52171516418457,
      "learning_rate": 4.65006729475101e-05,
      "loss": 0.7901,
      "step": 1300
    },
    {
      "epoch": 0.3633916554508748,
      "grad_norm": 9.326014518737793,
      "learning_rate": 4.6366083445491256e-05,
      "loss": 0.9077,
      "step": 1350
    },
    {
      "epoch": 0.3768506056527591,
      "grad_norm": 4.478005886077881,
      "learning_rate": 4.623149394347241e-05,
      "loss": 0.8236,
      "step": 1400
    },
    {
      "epoch": 0.39030955585464333,
      "grad_norm": 5.142934322357178,
      "learning_rate": 4.609690444145357e-05,
      "loss": 0.8649,
      "step": 1450
    },
    {
      "epoch": 0.4037685060565276,
      "grad_norm": 8.483355522155762,
      "learning_rate": 4.5962314939434725e-05,
      "loss": 0.9226,
      "step": 1500
    },
    {
      "epoch": 0.41722745625841184,
      "grad_norm": 5.120174884796143,
      "learning_rate": 4.582772543741589e-05,
      "loss": 0.811,
      "step": 1550
    },
    {
      "epoch": 0.4306864064602961,
      "grad_norm": 6.442200660705566,
      "learning_rate": 4.5693135935397044e-05,
      "loss": 0.8906,
      "step": 1600
    },
    {
      "epoch": 0.44414535666218036,
      "grad_norm": 8.625208854675293,
      "learning_rate": 4.5558546433378194e-05,
      "loss": 0.835,
      "step": 1650
    },
    {
      "epoch": 0.4576043068640646,
      "grad_norm": 4.066617012023926,
      "learning_rate": 4.542395693135936e-05,
      "loss": 0.8308,
      "step": 1700
    },
    {
      "epoch": 0.47106325706594887,
      "grad_norm": 3.4426660537719727,
      "learning_rate": 4.528936742934051e-05,
      "loss": 0.8209,
      "step": 1750
    },
    {
      "epoch": 0.4845222072678331,
      "grad_norm": 6.1950201988220215,
      "learning_rate": 4.5154777927321676e-05,
      "loss": 0.8583,
      "step": 1800
    },
    {
      "epoch": 0.4979811574697174,
      "grad_norm": 7.984889507293701,
      "learning_rate": 4.5020188425302826e-05,
      "loss": 0.8624,
      "step": 1850
    },
    {
      "epoch": 0.5114401076716016,
      "grad_norm": 8.470647811889648,
      "learning_rate": 4.488559892328398e-05,
      "loss": 0.8256,
      "step": 1900
    },
    {
      "epoch": 0.5248990578734859,
      "grad_norm": 3.995429754257202,
      "learning_rate": 4.4751009421265145e-05,
      "loss": 0.8389,
      "step": 1950
    },
    {
      "epoch": 0.5383580080753702,
      "grad_norm": 5.522305965423584,
      "learning_rate": 4.46164199192463e-05,
      "loss": 0.8273,
      "step": 2000
    },
    {
      "epoch": 0.5518169582772544,
      "grad_norm": 5.972984313964844,
      "learning_rate": 4.448183041722746e-05,
      "loss": 0.8401,
      "step": 2050
    },
    {
      "epoch": 0.5652759084791387,
      "grad_norm": 6.424256324768066,
      "learning_rate": 4.4347240915208614e-05,
      "loss": 0.8846,
      "step": 2100
    },
    {
      "epoch": 0.5787348586810229,
      "grad_norm": 4.666204452514648,
      "learning_rate": 4.421265141318977e-05,
      "loss": 0.843,
      "step": 2150
    },
    {
      "epoch": 0.5921938088829072,
      "grad_norm": 4.421188831329346,
      "learning_rate": 4.407806191117093e-05,
      "loss": 0.8415,
      "step": 2200
    },
    {
      "epoch": 0.6056527590847914,
      "grad_norm": 7.451023578643799,
      "learning_rate": 4.394347240915209e-05,
      "loss": 0.8582,
      "step": 2250
    },
    {
      "epoch": 0.6191117092866757,
      "grad_norm": 6.253117084503174,
      "learning_rate": 4.3808882907133246e-05,
      "loss": 0.864,
      "step": 2300
    },
    {
      "epoch": 0.6325706594885598,
      "grad_norm": 8.422286987304688,
      "learning_rate": 4.36742934051144e-05,
      "loss": 0.8129,
      "step": 2350
    },
    {
      "epoch": 0.6460296096904441,
      "grad_norm": 5.943587303161621,
      "learning_rate": 4.3539703903095565e-05,
      "loss": 0.777,
      "step": 2400
    },
    {
      "epoch": 0.6594885598923284,
      "grad_norm": 6.759542465209961,
      "learning_rate": 4.340511440107672e-05,
      "loss": 0.8953,
      "step": 2450
    },
    {
      "epoch": 0.6729475100942126,
      "grad_norm": 7.132166862487793,
      "learning_rate": 4.327052489905787e-05,
      "loss": 0.8718,
      "step": 2500
    },
    {
      "epoch": 0.6864064602960969,
      "grad_norm": 8.092341423034668,
      "learning_rate": 4.3135935397039034e-05,
      "loss": 0.8189,
      "step": 2550
    },
    {
      "epoch": 0.6998654104979811,
      "grad_norm": 8.999674797058105,
      "learning_rate": 4.300134589502019e-05,
      "loss": 0.8511,
      "step": 2600
    },
    {
      "epoch": 0.7133243606998654,
      "grad_norm": 5.900701999664307,
      "learning_rate": 4.286675639300135e-05,
      "loss": 0.8541,
      "step": 2650
    },
    {
      "epoch": 0.7267833109017496,
      "grad_norm": 3.098007917404175,
      "learning_rate": 4.27321668909825e-05,
      "loss": 0.7713,
      "step": 2700
    },
    {
      "epoch": 0.7402422611036339,
      "grad_norm": 9.590667724609375,
      "learning_rate": 4.259757738896366e-05,
      "loss": 0.8809,
      "step": 2750
    },
    {
      "epoch": 0.7537012113055181,
      "grad_norm": 6.4679036140441895,
      "learning_rate": 4.246298788694482e-05,
      "loss": 0.8199,
      "step": 2800
    },
    {
      "epoch": 0.7671601615074024,
      "grad_norm": 4.938276290893555,
      "learning_rate": 4.232839838492598e-05,
      "loss": 0.8078,
      "step": 2850
    },
    {
      "epoch": 0.7806191117092867,
      "grad_norm": 10.225153923034668,
      "learning_rate": 4.2193808882907135e-05,
      "loss": 0.8204,
      "step": 2900
    },
    {
      "epoch": 0.7940780619111709,
      "grad_norm": 3.3259975910186768,
      "learning_rate": 4.205921938088829e-05,
      "loss": 0.8467,
      "step": 2950
    },
    {
      "epoch": 0.8075370121130552,
      "grad_norm": 9.01834774017334,
      "learning_rate": 4.192462987886945e-05,
      "loss": 0.868,
      "step": 3000
    },
    {
      "epoch": 0.8209959623149394,
      "grad_norm": 6.5954437255859375,
      "learning_rate": 4.179004037685061e-05,
      "loss": 0.8652,
      "step": 3050
    },
    {
      "epoch": 0.8344549125168237,
      "grad_norm": 4.068777084350586,
      "learning_rate": 4.165545087483177e-05,
      "loss": 0.8118,
      "step": 3100
    },
    {
      "epoch": 0.847913862718708,
      "grad_norm": 17.435169219970703,
      "learning_rate": 4.152086137281292e-05,
      "loss": 0.871,
      "step": 3150
    },
    {
      "epoch": 0.8613728129205922,
      "grad_norm": 10.511228561401367,
      "learning_rate": 4.138627187079408e-05,
      "loss": 0.8494,
      "step": 3200
    },
    {
      "epoch": 0.8748317631224765,
      "grad_norm": 4.344993591308594,
      "learning_rate": 4.1251682368775236e-05,
      "loss": 0.8448,
      "step": 3250
    },
    {
      "epoch": 0.8882907133243607,
      "grad_norm": 4.828742504119873,
      "learning_rate": 4.11170928667564e-05,
      "loss": 0.7864,
      "step": 3300
    },
    {
      "epoch": 0.901749663526245,
      "grad_norm": 3.680612325668335,
      "learning_rate": 4.098250336473755e-05,
      "loss": 0.804,
      "step": 3350
    },
    {
      "epoch": 0.9152086137281292,
      "grad_norm": 5.0831828117370605,
      "learning_rate": 4.084791386271871e-05,
      "loss": 0.8439,
      "step": 3400
    },
    {
      "epoch": 0.9286675639300135,
      "grad_norm": 7.19814920425415,
      "learning_rate": 4.071332436069987e-05,
      "loss": 0.8224,
      "step": 3450
    },
    {
      "epoch": 0.9421265141318977,
      "grad_norm": 5.716472625732422,
      "learning_rate": 4.0578734858681024e-05,
      "loss": 0.8244,
      "step": 3500
    },
    {
      "epoch": 0.955585464333782,
      "grad_norm": 6.69605827331543,
      "learning_rate": 4.044414535666218e-05,
      "loss": 0.7773,
      "step": 3550
    },
    {
      "epoch": 0.9690444145356663,
      "grad_norm": 5.048390865325928,
      "learning_rate": 4.0309555854643336e-05,
      "loss": 0.8725,
      "step": 3600
    },
    {
      "epoch": 0.9825033647375505,
      "grad_norm": 7.14629602432251,
      "learning_rate": 4.01749663526245e-05,
      "loss": 0.8099,
      "step": 3650
    },
    {
      "epoch": 0.9959623149394348,
      "grad_norm": 6.234419822692871,
      "learning_rate": 4.0040376850605656e-05,
      "loss": 0.8708,
      "step": 3700
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6704805491990846,
      "eval_loss": 0.7913440465927124,
      "eval_runtime": 156.3593,
      "eval_samples_per_second": 33.538,
      "eval_steps_per_second": 4.195,
      "step": 3715
    },
    {
      "epoch": 1.009421265141319,
      "grad_norm": 7.098825454711914,
      "learning_rate": 3.990578734858681e-05,
      "loss": 0.8049,
      "step": 3750
    },
    {
      "epoch": 1.0228802153432033,
      "grad_norm": 3.8099076747894287,
      "learning_rate": 3.977119784656797e-05,
      "loss": 0.7764,
      "step": 3800
    },
    {
      "epoch": 1.0363391655450875,
      "grad_norm": 6.63611364364624,
      "learning_rate": 3.9636608344549125e-05,
      "loss": 0.8281,
      "step": 3850
    },
    {
      "epoch": 1.0497981157469718,
      "grad_norm": 6.808389663696289,
      "learning_rate": 3.950201884253029e-05,
      "loss": 0.8062,
      "step": 3900
    },
    {
      "epoch": 1.063257065948856,
      "grad_norm": 5.380650520324707,
      "learning_rate": 3.9367429340511444e-05,
      "loss": 0.8138,
      "step": 3950
    },
    {
      "epoch": 1.0767160161507403,
      "grad_norm": 5.80025053024292,
      "learning_rate": 3.92328398384926e-05,
      "loss": 0.8332,
      "step": 4000
    },
    {
      "epoch": 1.0901749663526246,
      "grad_norm": 4.020734786987305,
      "learning_rate": 3.9098250336473756e-05,
      "loss": 0.8567,
      "step": 4050
    },
    {
      "epoch": 1.1036339165545088,
      "grad_norm": 7.735576152801514,
      "learning_rate": 3.896366083445491e-05,
      "loss": 0.7845,
      "step": 4100
    },
    {
      "epoch": 1.117092866756393,
      "grad_norm": 4.9111480712890625,
      "learning_rate": 3.8829071332436076e-05,
      "loss": 0.7963,
      "step": 4150
    },
    {
      "epoch": 1.1305518169582773,
      "grad_norm": 4.221532821655273,
      "learning_rate": 3.8694481830417225e-05,
      "loss": 0.795,
      "step": 4200
    },
    {
      "epoch": 1.1440107671601616,
      "grad_norm": 11.667252540588379,
      "learning_rate": 3.855989232839839e-05,
      "loss": 0.8014,
      "step": 4250
    },
    {
      "epoch": 1.1574697173620458,
      "grad_norm": 4.788601875305176,
      "learning_rate": 3.8425302826379545e-05,
      "loss": 0.8098,
      "step": 4300
    },
    {
      "epoch": 1.17092866756393,
      "grad_norm": 4.632471084594727,
      "learning_rate": 3.82907133243607e-05,
      "loss": 0.7966,
      "step": 4350
    },
    {
      "epoch": 1.1843876177658144,
      "grad_norm": 5.864156723022461,
      "learning_rate": 3.815612382234186e-05,
      "loss": 0.8145,
      "step": 4400
    },
    {
      "epoch": 1.1978465679676986,
      "grad_norm": 4.13134241104126,
      "learning_rate": 3.8021534320323014e-05,
      "loss": 0.8005,
      "step": 4450
    },
    {
      "epoch": 1.2113055181695827,
      "grad_norm": 5.466054439544678,
      "learning_rate": 3.788694481830418e-05,
      "loss": 0.7934,
      "step": 4500
    },
    {
      "epoch": 1.224764468371467,
      "grad_norm": 10.55119514465332,
      "learning_rate": 3.775235531628533e-05,
      "loss": 0.793,
      "step": 4550
    },
    {
      "epoch": 1.2382234185733512,
      "grad_norm": 5.241918087005615,
      "learning_rate": 3.761776581426649e-05,
      "loss": 0.7923,
      "step": 4600
    },
    {
      "epoch": 1.2516823687752354,
      "grad_norm": 4.515138149261475,
      "learning_rate": 3.7483176312247645e-05,
      "loss": 0.7835,
      "step": 4650
    },
    {
      "epoch": 1.2651413189771197,
      "grad_norm": 4.1832594871521,
      "learning_rate": 3.73485868102288e-05,
      "loss": 0.7776,
      "step": 4700
    },
    {
      "epoch": 1.278600269179004,
      "grad_norm": 10.602478981018066,
      "learning_rate": 3.7213997308209965e-05,
      "loss": 0.7605,
      "step": 4750
    },
    {
      "epoch": 1.2920592193808882,
      "grad_norm": 5.472063064575195,
      "learning_rate": 3.707940780619112e-05,
      "loss": 0.7888,
      "step": 4800
    },
    {
      "epoch": 1.3055181695827724,
      "grad_norm": 5.9233856201171875,
      "learning_rate": 3.694481830417227e-05,
      "loss": 0.8342,
      "step": 4850
    },
    {
      "epoch": 1.3189771197846567,
      "grad_norm": 5.267497539520264,
      "learning_rate": 3.6810228802153434e-05,
      "loss": 0.8264,
      "step": 4900
    },
    {
      "epoch": 1.332436069986541,
      "grad_norm": 6.449831962585449,
      "learning_rate": 3.667563930013459e-05,
      "loss": 0.8355,
      "step": 4950
    },
    {
      "epoch": 1.3458950201884252,
      "grad_norm": 5.223786354064941,
      "learning_rate": 3.654104979811575e-05,
      "loss": 0.8524,
      "step": 5000
    },
    {
      "epoch": 1.3593539703903095,
      "grad_norm": 3.634272813796997,
      "learning_rate": 3.64064602960969e-05,
      "loss": 0.7662,
      "step": 5050
    },
    {
      "epoch": 1.3728129205921937,
      "grad_norm": 7.648930549621582,
      "learning_rate": 3.627187079407806e-05,
      "loss": 0.7881,
      "step": 5100
    },
    {
      "epoch": 1.386271870794078,
      "grad_norm": 3.6644184589385986,
      "learning_rate": 3.613728129205922e-05,
      "loss": 0.7209,
      "step": 5150
    },
    {
      "epoch": 1.3997308209959622,
      "grad_norm": 5.135678768157959,
      "learning_rate": 3.600269179004038e-05,
      "loss": 0.7973,
      "step": 5200
    },
    {
      "epoch": 1.4131897711978465,
      "grad_norm": 7.482494831085205,
      "learning_rate": 3.5868102288021534e-05,
      "loss": 0.7936,
      "step": 5250
    },
    {
      "epoch": 1.4266487213997308,
      "grad_norm": 6.0146098136901855,
      "learning_rate": 3.573351278600269e-05,
      "loss": 0.7863,
      "step": 5300
    },
    {
      "epoch": 1.440107671601615,
      "grad_norm": 4.237415790557861,
      "learning_rate": 3.559892328398385e-05,
      "loss": 0.8212,
      "step": 5350
    },
    {
      "epoch": 1.4535666218034993,
      "grad_norm": 5.485822677612305,
      "learning_rate": 3.546433378196501e-05,
      "loss": 0.7721,
      "step": 5400
    },
    {
      "epoch": 1.4670255720053835,
      "grad_norm": 7.599061489105225,
      "learning_rate": 3.5329744279946166e-05,
      "loss": 0.8363,
      "step": 5450
    },
    {
      "epoch": 1.4804845222072678,
      "grad_norm": 5.499344825744629,
      "learning_rate": 3.519515477792732e-05,
      "loss": 0.8356,
      "step": 5500
    },
    {
      "epoch": 1.493943472409152,
      "grad_norm": 4.69277811050415,
      "learning_rate": 3.506056527590848e-05,
      "loss": 0.8363,
      "step": 5550
    },
    {
      "epoch": 1.5074024226110363,
      "grad_norm": 5.5211968421936035,
      "learning_rate": 3.492597577388964e-05,
      "loss": 0.759,
      "step": 5600
    },
    {
      "epoch": 1.5208613728129206,
      "grad_norm": 4.382919788360596,
      "learning_rate": 3.47913862718708e-05,
      "loss": 0.8433,
      "step": 5650
    },
    {
      "epoch": 1.5343203230148048,
      "grad_norm": 4.304983615875244,
      "learning_rate": 3.4656796769851955e-05,
      "loss": 0.7769,
      "step": 5700
    },
    {
      "epoch": 1.547779273216689,
      "grad_norm": 4.251346588134766,
      "learning_rate": 3.452220726783311e-05,
      "loss": 0.7433,
      "step": 5750
    },
    {
      "epoch": 1.5612382234185733,
      "grad_norm": 6.309762001037598,
      "learning_rate": 3.438761776581427e-05,
      "loss": 0.7926,
      "step": 5800
    },
    {
      "epoch": 1.5746971736204576,
      "grad_norm": 10.575916290283203,
      "learning_rate": 3.425302826379543e-05,
      "loss": 0.7576,
      "step": 5850
    },
    {
      "epoch": 1.5881561238223418,
      "grad_norm": 5.088288307189941,
      "learning_rate": 3.4118438761776587e-05,
      "loss": 0.8118,
      "step": 5900
    },
    {
      "epoch": 1.601615074024226,
      "grad_norm": 9.777997970581055,
      "learning_rate": 3.3983849259757736e-05,
      "loss": 0.7294,
      "step": 5950
    },
    {
      "epoch": 1.6150740242261103,
      "grad_norm": 6.316093444824219,
      "learning_rate": 3.38492597577389e-05,
      "loss": 0.792,
      "step": 6000
    },
    {
      "epoch": 1.6285329744279946,
      "grad_norm": 3.973142385482788,
      "learning_rate": 3.3714670255720055e-05,
      "loss": 0.7887,
      "step": 6050
    },
    {
      "epoch": 1.6419919246298789,
      "grad_norm": 5.73635721206665,
      "learning_rate": 3.358008075370122e-05,
      "loss": 0.7605,
      "step": 6100
    },
    {
      "epoch": 1.6554508748317631,
      "grad_norm": 6.965853214263916,
      "learning_rate": 3.344549125168237e-05,
      "loss": 0.7346,
      "step": 6150
    },
    {
      "epoch": 1.6689098250336474,
      "grad_norm": 4.421396255493164,
      "learning_rate": 3.3310901749663524e-05,
      "loss": 0.8395,
      "step": 6200
    },
    {
      "epoch": 1.6823687752355316,
      "grad_norm": 4.829565525054932,
      "learning_rate": 3.317631224764469e-05,
      "loss": 0.7574,
      "step": 6250
    },
    {
      "epoch": 1.695827725437416,
      "grad_norm": 5.747725963592529,
      "learning_rate": 3.3041722745625844e-05,
      "loss": 0.7673,
      "step": 6300
    },
    {
      "epoch": 1.7092866756393001,
      "grad_norm": 5.420334815979004,
      "learning_rate": 3.2907133243607e-05,
      "loss": 0.7435,
      "step": 6350
    },
    {
      "epoch": 1.7227456258411844,
      "grad_norm": 4.195629119873047,
      "learning_rate": 3.2772543741588156e-05,
      "loss": 0.7451,
      "step": 6400
    },
    {
      "epoch": 1.7362045760430687,
      "grad_norm": 6.293215274810791,
      "learning_rate": 3.263795423956931e-05,
      "loss": 0.7053,
      "step": 6450
    },
    {
      "epoch": 1.749663526244953,
      "grad_norm": 7.165382385253906,
      "learning_rate": 3.2503364737550476e-05,
      "loss": 0.8185,
      "step": 6500
    },
    {
      "epoch": 1.7631224764468372,
      "grad_norm": 7.3188605308532715,
      "learning_rate": 3.236877523553163e-05,
      "loss": 0.8172,
      "step": 6550
    },
    {
      "epoch": 1.7765814266487214,
      "grad_norm": 9.398101806640625,
      "learning_rate": 3.223418573351279e-05,
      "loss": 0.8222,
      "step": 6600
    },
    {
      "epoch": 1.7900403768506057,
      "grad_norm": 3.5556681156158447,
      "learning_rate": 3.2099596231493944e-05,
      "loss": 0.6997,
      "step": 6650
    },
    {
      "epoch": 1.80349932705249,
      "grad_norm": 4.965388298034668,
      "learning_rate": 3.19650067294751e-05,
      "loss": 0.8282,
      "step": 6700
    },
    {
      "epoch": 1.8169582772543742,
      "grad_norm": 6.920792579650879,
      "learning_rate": 3.1830417227456264e-05,
      "loss": 0.8008,
      "step": 6750
    },
    {
      "epoch": 1.8304172274562585,
      "grad_norm": 5.137085437774658,
      "learning_rate": 3.169582772543741e-05,
      "loss": 0.7904,
      "step": 6800
    },
    {
      "epoch": 1.8438761776581427,
      "grad_norm": 3.463829278945923,
      "learning_rate": 3.1561238223418576e-05,
      "loss": 0.8005,
      "step": 6850
    },
    {
      "epoch": 1.857335127860027,
      "grad_norm": 8.965896606445312,
      "learning_rate": 3.142664872139973e-05,
      "loss": 0.7587,
      "step": 6900
    },
    {
      "epoch": 1.8707940780619112,
      "grad_norm": 2.6511034965515137,
      "learning_rate": 3.129205921938089e-05,
      "loss": 0.8108,
      "step": 6950
    },
    {
      "epoch": 1.8842530282637955,
      "grad_norm": 5.621224403381348,
      "learning_rate": 3.1157469717362045e-05,
      "loss": 0.7632,
      "step": 7000
    },
    {
      "epoch": 1.8977119784656797,
      "grad_norm": 5.562582492828369,
      "learning_rate": 3.10228802153432e-05,
      "loss": 0.7778,
      "step": 7050
    },
    {
      "epoch": 1.911170928667564,
      "grad_norm": 5.42648983001709,
      "learning_rate": 3.0888290713324365e-05,
      "loss": 0.8093,
      "step": 7100
    },
    {
      "epoch": 1.9246298788694483,
      "grad_norm": 4.3708367347717285,
      "learning_rate": 3.075370121130552e-05,
      "loss": 0.756,
      "step": 7150
    },
    {
      "epoch": 1.9380888290713325,
      "grad_norm": 8.614686012268066,
      "learning_rate": 3.061911170928668e-05,
      "loss": 0.7812,
      "step": 7200
    },
    {
      "epoch": 1.9515477792732168,
      "grad_norm": 5.445189476013184,
      "learning_rate": 3.0484522207267833e-05,
      "loss": 0.7345,
      "step": 7250
    },
    {
      "epoch": 1.965006729475101,
      "grad_norm": 3.8735148906707764,
      "learning_rate": 3.034993270524899e-05,
      "loss": 0.7543,
      "step": 7300
    },
    {
      "epoch": 1.9784656796769853,
      "grad_norm": 7.67860221862793,
      "learning_rate": 3.021534320323015e-05,
      "loss": 0.7844,
      "step": 7350
    },
    {
      "epoch": 1.9919246298788695,
      "grad_norm": 6.188866138458252,
      "learning_rate": 3.0080753701211306e-05,
      "loss": 0.7174,
      "step": 7400
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6834477498093059,
      "eval_loss": 0.7695640921592712,
      "eval_runtime": 155.7659,
      "eval_samples_per_second": 33.666,
      "eval_steps_per_second": 4.211,
      "step": 7430
    },
    {
      "epoch": 2.005383580080754,
      "grad_norm": 4.633266448974609,
      "learning_rate": 2.9946164199192465e-05,
      "loss": 0.7402,
      "step": 7450
    },
    {
      "epoch": 2.018842530282638,
      "grad_norm": 6.980320453643799,
      "learning_rate": 2.981157469717362e-05,
      "loss": 0.7418,
      "step": 7500
    },
    {
      "epoch": 2.0323014804845223,
      "grad_norm": 4.257971286773682,
      "learning_rate": 2.9676985195154778e-05,
      "loss": 0.7766,
      "step": 7550
    },
    {
      "epoch": 2.0457604306864066,
      "grad_norm": 7.159633636474609,
      "learning_rate": 2.9542395693135938e-05,
      "loss": 0.7895,
      "step": 7600
    },
    {
      "epoch": 2.059219380888291,
      "grad_norm": 8.745402336120605,
      "learning_rate": 2.9407806191117094e-05,
      "loss": 0.7593,
      "step": 7650
    },
    {
      "epoch": 2.072678331090175,
      "grad_norm": 5.013448715209961,
      "learning_rate": 2.9273216689098254e-05,
      "loss": 0.7182,
      "step": 7700
    },
    {
      "epoch": 2.0861372812920593,
      "grad_norm": 7.671382904052734,
      "learning_rate": 2.913862718707941e-05,
      "loss": 0.7914,
      "step": 7750
    },
    {
      "epoch": 2.0995962314939436,
      "grad_norm": 5.53889274597168,
      "learning_rate": 2.9004037685060566e-05,
      "loss": 0.7335,
      "step": 7800
    },
    {
      "epoch": 2.113055181695828,
      "grad_norm": 7.8011298179626465,
      "learning_rate": 2.8869448183041726e-05,
      "loss": 0.8467,
      "step": 7850
    },
    {
      "epoch": 2.126514131897712,
      "grad_norm": 8.303256034851074,
      "learning_rate": 2.8734858681022882e-05,
      "loss": 0.8296,
      "step": 7900
    },
    {
      "epoch": 2.1399730820995964,
      "grad_norm": 3.7274887561798096,
      "learning_rate": 2.8600269179004042e-05,
      "loss": 0.81,
      "step": 7950
    },
    {
      "epoch": 2.1534320323014806,
      "grad_norm": 4.304510116577148,
      "learning_rate": 2.8465679676985198e-05,
      "loss": 0.7638,
      "step": 8000
    },
    {
      "epoch": 2.166890982503365,
      "grad_norm": 6.158349514007568,
      "learning_rate": 2.833109017496635e-05,
      "loss": 0.7646,
      "step": 8050
    },
    {
      "epoch": 2.180349932705249,
      "grad_norm": 8.251932144165039,
      "learning_rate": 2.8196500672947514e-05,
      "loss": 0.7754,
      "step": 8100
    },
    {
      "epoch": 2.1938088829071334,
      "grad_norm": 6.835890769958496,
      "learning_rate": 2.8061911170928667e-05,
      "loss": 0.7115,
      "step": 8150
    },
    {
      "epoch": 2.2072678331090176,
      "grad_norm": 7.077799320220947,
      "learning_rate": 2.792732166890983e-05,
      "loss": 0.7574,
      "step": 8200
    },
    {
      "epoch": 2.220726783310902,
      "grad_norm": 4.870689392089844,
      "learning_rate": 2.7792732166890983e-05,
      "loss": 0.7163,
      "step": 8250
    },
    {
      "epoch": 2.234185733512786,
      "grad_norm": 5.488121509552002,
      "learning_rate": 2.765814266487214e-05,
      "loss": 0.6895,
      "step": 8300
    },
    {
      "epoch": 2.2476446837146704,
      "grad_norm": 4.924771785736084,
      "learning_rate": 2.75235531628533e-05,
      "loss": 0.6974,
      "step": 8350
    },
    {
      "epoch": 2.2611036339165547,
      "grad_norm": 4.229039669036865,
      "learning_rate": 2.7388963660834455e-05,
      "loss": 0.7773,
      "step": 8400
    },
    {
      "epoch": 2.274562584118439,
      "grad_norm": 6.898426055908203,
      "learning_rate": 2.7254374158815615e-05,
      "loss": 0.7666,
      "step": 8450
    },
    {
      "epoch": 2.288021534320323,
      "grad_norm": 6.882368087768555,
      "learning_rate": 2.711978465679677e-05,
      "loss": 0.7854,
      "step": 8500
    },
    {
      "epoch": 2.3014804845222074,
      "grad_norm": 4.808278560638428,
      "learning_rate": 2.698519515477793e-05,
      "loss": 0.7758,
      "step": 8550
    },
    {
      "epoch": 2.3149394347240917,
      "grad_norm": 3.1910240650177,
      "learning_rate": 2.6850605652759087e-05,
      "loss": 0.7461,
      "step": 8600
    },
    {
      "epoch": 2.328398384925976,
      "grad_norm": 5.628363609313965,
      "learning_rate": 2.6716016150740243e-05,
      "loss": 0.7429,
      "step": 8650
    },
    {
      "epoch": 2.34185733512786,
      "grad_norm": 5.283363342285156,
      "learning_rate": 2.6581426648721403e-05,
      "loss": 0.8075,
      "step": 8700
    },
    {
      "epoch": 2.3553162853297445,
      "grad_norm": 6.233378887176514,
      "learning_rate": 2.644683714670256e-05,
      "loss": 0.8115,
      "step": 8750
    },
    {
      "epoch": 2.3687752355316287,
      "grad_norm": 6.880258083343506,
      "learning_rate": 2.631224764468372e-05,
      "loss": 0.7468,
      "step": 8800
    },
    {
      "epoch": 2.382234185733513,
      "grad_norm": 4.263345718383789,
      "learning_rate": 2.6177658142664875e-05,
      "loss": 0.7975,
      "step": 8850
    },
    {
      "epoch": 2.3956931359353972,
      "grad_norm": 5.902066707611084,
      "learning_rate": 2.6043068640646028e-05,
      "loss": 0.7657,
      "step": 8900
    },
    {
      "epoch": 2.409152086137281,
      "grad_norm": 8.192139625549316,
      "learning_rate": 2.590847913862719e-05,
      "loss": 0.7572,
      "step": 8950
    },
    {
      "epoch": 2.4226110363391653,
      "grad_norm": 4.5798869132995605,
      "learning_rate": 2.5773889636608344e-05,
      "loss": 0.7399,
      "step": 9000
    },
    {
      "epoch": 2.4360699865410496,
      "grad_norm": 5.326756954193115,
      "learning_rate": 2.5639300134589507e-05,
      "loss": 0.7611,
      "step": 9050
    },
    {
      "epoch": 2.449528936742934,
      "grad_norm": 8.937490463256836,
      "learning_rate": 2.550471063257066e-05,
      "loss": 0.7099,
      "step": 9100
    },
    {
      "epoch": 2.462987886944818,
      "grad_norm": 9.286219596862793,
      "learning_rate": 2.5370121130551816e-05,
      "loss": 0.6846,
      "step": 9150
    },
    {
      "epoch": 2.4764468371467023,
      "grad_norm": 5.3273444175720215,
      "learning_rate": 2.5235531628532976e-05,
      "loss": 0.8164,
      "step": 9200
    },
    {
      "epoch": 2.4899057873485866,
      "grad_norm": 5.835832118988037,
      "learning_rate": 2.5100942126514132e-05,
      "loss": 0.7169,
      "step": 9250
    },
    {
      "epoch": 2.503364737550471,
      "grad_norm": 10.228128433227539,
      "learning_rate": 2.496635262449529e-05,
      "loss": 0.7451,
      "step": 9300
    },
    {
      "epoch": 2.516823687752355,
      "grad_norm": 6.691425323486328,
      "learning_rate": 2.4831763122476448e-05,
      "loss": 0.8068,
      "step": 9350
    },
    {
      "epoch": 2.5302826379542394,
      "grad_norm": 12.391800880432129,
      "learning_rate": 2.4697173620457605e-05,
      "loss": 0.8208,
      "step": 9400
    },
    {
      "epoch": 2.5437415881561236,
      "grad_norm": 6.797051429748535,
      "learning_rate": 2.4562584118438764e-05,
      "loss": 0.687,
      "step": 9450
    },
    {
      "epoch": 2.557200538358008,
      "grad_norm": 7.98144006729126,
      "learning_rate": 2.442799461641992e-05,
      "loss": 0.7773,
      "step": 9500
    },
    {
      "epoch": 2.570659488559892,
      "grad_norm": 6.579287528991699,
      "learning_rate": 2.4293405114401077e-05,
      "loss": 0.7479,
      "step": 9550
    },
    {
      "epoch": 2.5841184387617764,
      "grad_norm": 8.774742126464844,
      "learning_rate": 2.4158815612382236e-05,
      "loss": 0.717,
      "step": 9600
    },
    {
      "epoch": 2.5975773889636606,
      "grad_norm": 6.015329360961914,
      "learning_rate": 2.4024226110363393e-05,
      "loss": 0.7697,
      "step": 9650
    },
    {
      "epoch": 2.611036339165545,
      "grad_norm": 6.891869068145752,
      "learning_rate": 2.3889636608344552e-05,
      "loss": 0.7398,
      "step": 9700
    },
    {
      "epoch": 2.624495289367429,
      "grad_norm": 3.5194456577301025,
      "learning_rate": 2.375504710632571e-05,
      "loss": 0.7388,
      "step": 9750
    },
    {
      "epoch": 2.6379542395693134,
      "grad_norm": 5.617457866668701,
      "learning_rate": 2.3620457604306865e-05,
      "loss": 0.7579,
      "step": 9800
    },
    {
      "epoch": 2.6514131897711977,
      "grad_norm": 3.8237452507019043,
      "learning_rate": 2.348586810228802e-05,
      "loss": 0.7117,
      "step": 9850
    },
    {
      "epoch": 2.664872139973082,
      "grad_norm": 8.812305450439453,
      "learning_rate": 2.335127860026918e-05,
      "loss": 0.7989,
      "step": 9900
    },
    {
      "epoch": 2.678331090174966,
      "grad_norm": 9.164283752441406,
      "learning_rate": 2.3216689098250337e-05,
      "loss": 0.7985,
      "step": 9950
    },
    {
      "epoch": 2.6917900403768504,
      "grad_norm": 4.9082818031311035,
      "learning_rate": 2.3082099596231497e-05,
      "loss": 0.6984,
      "step": 10000
    },
    {
      "epoch": 2.7052489905787347,
      "grad_norm": 10.995796203613281,
      "learning_rate": 2.294751009421265e-05,
      "loss": 0.6436,
      "step": 10050
    },
    {
      "epoch": 2.718707940780619,
      "grad_norm": 7.530245304107666,
      "learning_rate": 2.281292059219381e-05,
      "loss": 0.8314,
      "step": 10100
    },
    {
      "epoch": 2.732166890982503,
      "grad_norm": 4.617403984069824,
      "learning_rate": 2.2678331090174966e-05,
      "loss": 0.7655,
      "step": 10150
    },
    {
      "epoch": 2.7456258411843875,
      "grad_norm": 2.954846143722534,
      "learning_rate": 2.2543741588156125e-05,
      "loss": 0.7366,
      "step": 10200
    },
    {
      "epoch": 2.7590847913862717,
      "grad_norm": 9.000774383544922,
      "learning_rate": 2.2409152086137282e-05,
      "loss": 0.817,
      "step": 10250
    },
    {
      "epoch": 2.772543741588156,
      "grad_norm": 5.622308731079102,
      "learning_rate": 2.2274562584118438e-05,
      "loss": 0.8301,
      "step": 10300
    },
    {
      "epoch": 2.7860026917900402,
      "grad_norm": 7.739029407501221,
      "learning_rate": 2.2139973082099598e-05,
      "loss": 0.7768,
      "step": 10350
    },
    {
      "epoch": 2.7994616419919245,
      "grad_norm": 7.988763809204102,
      "learning_rate": 2.2005383580080754e-05,
      "loss": 0.7636,
      "step": 10400
    },
    {
      "epoch": 2.8129205921938087,
      "grad_norm": 8.64675235748291,
      "learning_rate": 2.1870794078061914e-05,
      "loss": 0.68,
      "step": 10450
    },
    {
      "epoch": 2.826379542395693,
      "grad_norm": 3.8631317615509033,
      "learning_rate": 2.173620457604307e-05,
      "loss": 0.7471,
      "step": 10500
    },
    {
      "epoch": 2.8398384925975773,
      "grad_norm": 9.414060592651367,
      "learning_rate": 2.160161507402423e-05,
      "loss": 0.7321,
      "step": 10550
    },
    {
      "epoch": 2.8532974427994615,
      "grad_norm": 6.183080673217773,
      "learning_rate": 2.1467025572005383e-05,
      "loss": 0.7832,
      "step": 10600
    },
    {
      "epoch": 2.8667563930013458,
      "grad_norm": 6.191742897033691,
      "learning_rate": 2.1332436069986542e-05,
      "loss": 0.7988,
      "step": 10650
    },
    {
      "epoch": 2.88021534320323,
      "grad_norm": 5.138295650482178,
      "learning_rate": 2.11978465679677e-05,
      "loss": 0.762,
      "step": 10700
    },
    {
      "epoch": 2.8936742934051143,
      "grad_norm": 3.5450708866119385,
      "learning_rate": 2.1063257065948858e-05,
      "loss": 0.7274,
      "step": 10750
    },
    {
      "epoch": 2.9071332436069985,
      "grad_norm": 6.234767913818359,
      "learning_rate": 2.0928667563930014e-05,
      "loss": 0.7913,
      "step": 10800
    },
    {
      "epoch": 2.920592193808883,
      "grad_norm": 9.082449913024902,
      "learning_rate": 2.079407806191117e-05,
      "loss": 0.7913,
      "step": 10850
    },
    {
      "epoch": 2.934051144010767,
      "grad_norm": 7.586862087249756,
      "learning_rate": 2.065948855989233e-05,
      "loss": 0.7619,
      "step": 10900
    },
    {
      "epoch": 2.9475100942126513,
      "grad_norm": 4.828477382659912,
      "learning_rate": 2.0524899057873487e-05,
      "loss": 0.7066,
      "step": 10950
    },
    {
      "epoch": 2.9609690444145356,
      "grad_norm": 5.221336364746094,
      "learning_rate": 2.0390309555854646e-05,
      "loss": 0.7338,
      "step": 11000
    },
    {
      "epoch": 2.97442799461642,
      "grad_norm": 4.758957386016846,
      "learning_rate": 2.0255720053835803e-05,
      "loss": 0.8249,
      "step": 11050
    },
    {
      "epoch": 2.987886944818304,
      "grad_norm": 7.711114883422852,
      "learning_rate": 2.012113055181696e-05,
      "loss": 0.701,
      "step": 11100
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.6876430205949656,
      "eval_loss": 0.7561132311820984,
      "eval_runtime": 155.7862,
      "eval_samples_per_second": 33.662,
      "eval_steps_per_second": 4.211,
      "step": 11145
    },
    {
      "epoch": 3.0013458950201883,
      "grad_norm": 4.683543682098389,
      "learning_rate": 1.9986541049798115e-05,
      "loss": 0.7394,
      "step": 11150
    },
    {
      "epoch": 3.0148048452220726,
      "grad_norm": 8.039644241333008,
      "learning_rate": 1.9851951547779275e-05,
      "loss": 0.7014,
      "step": 11200
    },
    {
      "epoch": 3.028263795423957,
      "grad_norm": 3.2450497150421143,
      "learning_rate": 1.971736204576043e-05,
      "loss": 0.7296,
      "step": 11250
    },
    {
      "epoch": 3.041722745625841,
      "grad_norm": 7.583424091339111,
      "learning_rate": 1.958277254374159e-05,
      "loss": 0.6854,
      "step": 11300
    },
    {
      "epoch": 3.0551816958277254,
      "grad_norm": 8.209442138671875,
      "learning_rate": 1.9448183041722747e-05,
      "loss": 0.7381,
      "step": 11350
    },
    {
      "epoch": 3.0686406460296096,
      "grad_norm": 5.490765571594238,
      "learning_rate": 1.9313593539703903e-05,
      "loss": 0.6621,
      "step": 11400
    },
    {
      "epoch": 3.082099596231494,
      "grad_norm": 3.6892144680023193,
      "learning_rate": 1.917900403768506e-05,
      "loss": 0.8162,
      "step": 11450
    },
    {
      "epoch": 3.095558546433378,
      "grad_norm": 3.7526772022247314,
      "learning_rate": 1.904441453566622e-05,
      "loss": 0.6856,
      "step": 11500
    },
    {
      "epoch": 3.1090174966352624,
      "grad_norm": 6.9756574630737305,
      "learning_rate": 1.8909825033647376e-05,
      "loss": 0.6925,
      "step": 11550
    },
    {
      "epoch": 3.1224764468371466,
      "grad_norm": 7.526263236999512,
      "learning_rate": 1.8775235531628535e-05,
      "loss": 0.7456,
      "step": 11600
    },
    {
      "epoch": 3.135935397039031,
      "grad_norm": 6.813902378082275,
      "learning_rate": 1.864064602960969e-05,
      "loss": 0.7249,
      "step": 11650
    },
    {
      "epoch": 3.149394347240915,
      "grad_norm": 9.295470237731934,
      "learning_rate": 1.8506056527590848e-05,
      "loss": 0.7712,
      "step": 11700
    },
    {
      "epoch": 3.1628532974427994,
      "grad_norm": 8.52511978149414,
      "learning_rate": 1.8371467025572008e-05,
      "loss": 0.719,
      "step": 11750
    },
    {
      "epoch": 3.1763122476446837,
      "grad_norm": 4.229515075683594,
      "learning_rate": 1.8236877523553164e-05,
      "loss": 0.8037,
      "step": 11800
    },
    {
      "epoch": 3.189771197846568,
      "grad_norm": 4.898521423339844,
      "learning_rate": 1.8102288021534324e-05,
      "loss": 0.7899,
      "step": 11850
    },
    {
      "epoch": 3.203230148048452,
      "grad_norm": 4.52465295791626,
      "learning_rate": 1.7967698519515476e-05,
      "loss": 0.6751,
      "step": 11900
    },
    {
      "epoch": 3.2166890982503364,
      "grad_norm": 10.052132606506348,
      "learning_rate": 1.7833109017496636e-05,
      "loss": 0.7059,
      "step": 11950
    },
    {
      "epoch": 3.2301480484522207,
      "grad_norm": 5.7466607093811035,
      "learning_rate": 1.7698519515477792e-05,
      "loss": 0.7111,
      "step": 12000
    },
    {
      "epoch": 3.243606998654105,
      "grad_norm": 6.689388275146484,
      "learning_rate": 1.7563930013458952e-05,
      "loss": 0.7243,
      "step": 12050
    },
    {
      "epoch": 3.257065948855989,
      "grad_norm": 5.682779788970947,
      "learning_rate": 1.742934051144011e-05,
      "loss": 0.7247,
      "step": 12100
    },
    {
      "epoch": 3.2705248990578735,
      "grad_norm": 8.503684043884277,
      "learning_rate": 1.7294751009421268e-05,
      "loss": 0.7941,
      "step": 12150
    },
    {
      "epoch": 3.2839838492597577,
      "grad_norm": 6.870131969451904,
      "learning_rate": 1.716016150740242e-05,
      "loss": 0.7602,
      "step": 12200
    },
    {
      "epoch": 3.297442799461642,
      "grad_norm": 6.552380084991455,
      "learning_rate": 1.702557200538358e-05,
      "loss": 0.735,
      "step": 12250
    },
    {
      "epoch": 3.3109017496635262,
      "grad_norm": 4.789989948272705,
      "learning_rate": 1.6890982503364737e-05,
      "loss": 0.7364,
      "step": 12300
    },
    {
      "epoch": 3.3243606998654105,
      "grad_norm": 8.60232162475586,
      "learning_rate": 1.6756393001345897e-05,
      "loss": 0.7947,
      "step": 12350
    },
    {
      "epoch": 3.3378196500672948,
      "grad_norm": 4.5044474601745605,
      "learning_rate": 1.6621803499327053e-05,
      "loss": 0.7338,
      "step": 12400
    },
    {
      "epoch": 3.351278600269179,
      "grad_norm": 6.923232078552246,
      "learning_rate": 1.648721399730821e-05,
      "loss": 0.7665,
      "step": 12450
    },
    {
      "epoch": 3.3647375504710633,
      "grad_norm": 3.56069016456604,
      "learning_rate": 1.635262449528937e-05,
      "loss": 0.6888,
      "step": 12500
    },
    {
      "epoch": 3.3781965006729475,
      "grad_norm": 6.3910417556762695,
      "learning_rate": 1.6218034993270525e-05,
      "loss": 0.7231,
      "step": 12550
    },
    {
      "epoch": 3.391655450874832,
      "grad_norm": 4.9129486083984375,
      "learning_rate": 1.6083445491251685e-05,
      "loss": 0.7561,
      "step": 12600
    },
    {
      "epoch": 3.405114401076716,
      "grad_norm": 6.198370933532715,
      "learning_rate": 1.594885598923284e-05,
      "loss": 0.7624,
      "step": 12650
    },
    {
      "epoch": 3.4185733512786003,
      "grad_norm": 3.865849494934082,
      "learning_rate": 1.5814266487213997e-05,
      "loss": 0.7848,
      "step": 12700
    },
    {
      "epoch": 3.4320323014804845,
      "grad_norm": 8.187667846679688,
      "learning_rate": 1.5679676985195154e-05,
      "loss": 0.6975,
      "step": 12750
    },
    {
      "epoch": 3.445491251682369,
      "grad_norm": 4.313286304473877,
      "learning_rate": 1.5545087483176313e-05,
      "loss": 0.6747,
      "step": 12800
    },
    {
      "epoch": 3.458950201884253,
      "grad_norm": 4.185740947723389,
      "learning_rate": 1.541049798115747e-05,
      "loss": 0.7172,
      "step": 12850
    },
    {
      "epoch": 3.4724091520861373,
      "grad_norm": 8.116366386413574,
      "learning_rate": 1.527590847913863e-05,
      "loss": 0.6771,
      "step": 12900
    },
    {
      "epoch": 3.4858681022880216,
      "grad_norm": 7.339372634887695,
      "learning_rate": 1.5141318977119787e-05,
      "loss": 0.7545,
      "step": 12950
    },
    {
      "epoch": 3.499327052489906,
      "grad_norm": 5.685112953186035,
      "learning_rate": 1.5006729475100942e-05,
      "loss": 0.7554,
      "step": 13000
    },
    {
      "epoch": 3.51278600269179,
      "grad_norm": 6.660563945770264,
      "learning_rate": 1.48721399730821e-05,
      "loss": 0.8129,
      "step": 13050
    },
    {
      "epoch": 3.5262449528936743,
      "grad_norm": 5.765139579772949,
      "learning_rate": 1.4737550471063258e-05,
      "loss": 0.7658,
      "step": 13100
    },
    {
      "epoch": 3.5397039030955586,
      "grad_norm": 13.765531539916992,
      "learning_rate": 1.4602960969044416e-05,
      "loss": 0.796,
      "step": 13150
    },
    {
      "epoch": 3.553162853297443,
      "grad_norm": 5.725189208984375,
      "learning_rate": 1.4468371467025574e-05,
      "loss": 0.7066,
      "step": 13200
    },
    {
      "epoch": 3.566621803499327,
      "grad_norm": 7.340361595153809,
      "learning_rate": 1.4333781965006728e-05,
      "loss": 0.703,
      "step": 13250
    },
    {
      "epoch": 3.5800807537012114,
      "grad_norm": 5.411958694458008,
      "learning_rate": 1.4199192462987886e-05,
      "loss": 0.7483,
      "step": 13300
    },
    {
      "epoch": 3.5935397039030956,
      "grad_norm": 6.428442001342773,
      "learning_rate": 1.4064602960969044e-05,
      "loss": 0.6997,
      "step": 13350
    },
    {
      "epoch": 3.60699865410498,
      "grad_norm": 4.238302707672119,
      "learning_rate": 1.3930013458950202e-05,
      "loss": 0.7664,
      "step": 13400
    },
    {
      "epoch": 3.620457604306864,
      "grad_norm": 8.822835922241211,
      "learning_rate": 1.379542395693136e-05,
      "loss": 0.7175,
      "step": 13450
    },
    {
      "epoch": 3.6339165545087484,
      "grad_norm": 6.3450140953063965,
      "learning_rate": 1.3660834454912517e-05,
      "loss": 0.7778,
      "step": 13500
    },
    {
      "epoch": 3.6473755047106327,
      "grad_norm": 6.263916015625,
      "learning_rate": 1.3526244952893675e-05,
      "loss": 0.7101,
      "step": 13550
    },
    {
      "epoch": 3.660834454912517,
      "grad_norm": 10.55626106262207,
      "learning_rate": 1.3391655450874833e-05,
      "loss": 0.7905,
      "step": 13600
    },
    {
      "epoch": 3.674293405114401,
      "grad_norm": 4.925227642059326,
      "learning_rate": 1.325706594885599e-05,
      "loss": 0.7279,
      "step": 13650
    },
    {
      "epoch": 3.6877523553162854,
      "grad_norm": 6.561845779418945,
      "learning_rate": 1.3122476446837149e-05,
      "loss": 0.6963,
      "step": 13700
    },
    {
      "epoch": 3.7012113055181697,
      "grad_norm": 3.45190691947937,
      "learning_rate": 1.2987886944818307e-05,
      "loss": 0.7425,
      "step": 13750
    },
    {
      "epoch": 3.714670255720054,
      "grad_norm": 7.690961837768555,
      "learning_rate": 1.2853297442799461e-05,
      "loss": 0.773,
      "step": 13800
    },
    {
      "epoch": 3.728129205921938,
      "grad_norm": 5.713979721069336,
      "learning_rate": 1.2718707940780619e-05,
      "loss": 0.7301,
      "step": 13850
    },
    {
      "epoch": 3.7415881561238225,
      "grad_norm": 5.182380199432373,
      "learning_rate": 1.2584118438761777e-05,
      "loss": 0.7388,
      "step": 13900
    },
    {
      "epoch": 3.7550471063257067,
      "grad_norm": 6.10720157623291,
      "learning_rate": 1.2449528936742935e-05,
      "loss": 0.7396,
      "step": 13950
    },
    {
      "epoch": 3.768506056527591,
      "grad_norm": 5.809687614440918,
      "learning_rate": 1.2314939434724091e-05,
      "loss": 0.758,
      "step": 14000
    },
    {
      "epoch": 3.781965006729475,
      "grad_norm": 7.089550495147705,
      "learning_rate": 1.218034993270525e-05,
      "loss": 0.704,
      "step": 14050
    },
    {
      "epoch": 3.7954239569313595,
      "grad_norm": 9.887908935546875,
      "learning_rate": 1.2045760430686406e-05,
      "loss": 0.6943,
      "step": 14100
    },
    {
      "epoch": 3.8088829071332437,
      "grad_norm": 6.512487888336182,
      "learning_rate": 1.1911170928667564e-05,
      "loss": 0.7531,
      "step": 14150
    },
    {
      "epoch": 3.822341857335128,
      "grad_norm": 6.52460241317749,
      "learning_rate": 1.1776581426648722e-05,
      "loss": 0.7855,
      "step": 14200
    },
    {
      "epoch": 3.8358008075370122,
      "grad_norm": 4.596036911010742,
      "learning_rate": 1.164199192462988e-05,
      "loss": 0.6761,
      "step": 14250
    },
    {
      "epoch": 3.8492597577388965,
      "grad_norm": 7.709405899047852,
      "learning_rate": 1.1507402422611038e-05,
      "loss": 0.8332,
      "step": 14300
    },
    {
      "epoch": 3.8627187079407808,
      "grad_norm": 3.506387710571289,
      "learning_rate": 1.1372812920592196e-05,
      "loss": 0.7692,
      "step": 14350
    },
    {
      "epoch": 3.876177658142665,
      "grad_norm": 4.950629711151123,
      "learning_rate": 1.1238223418573352e-05,
      "loss": 0.7818,
      "step": 14400
    },
    {
      "epoch": 3.8896366083445493,
      "grad_norm": 3.943636417388916,
      "learning_rate": 1.110363391655451e-05,
      "loss": 0.7512,
      "step": 14450
    },
    {
      "epoch": 3.9030955585464335,
      "grad_norm": 4.775928497314453,
      "learning_rate": 1.0969044414535666e-05,
      "loss": 0.7389,
      "step": 14500
    },
    {
      "epoch": 3.916554508748318,
      "grad_norm": 5.737483978271484,
      "learning_rate": 1.0834454912516824e-05,
      "loss": 0.7571,
      "step": 14550
    },
    {
      "epoch": 3.930013458950202,
      "grad_norm": 7.8546600341796875,
      "learning_rate": 1.0699865410497982e-05,
      "loss": 0.7946,
      "step": 14600
    },
    {
      "epoch": 3.9434724091520863,
      "grad_norm": 6.826828956604004,
      "learning_rate": 1.0565275908479138e-05,
      "loss": 0.7428,
      "step": 14650
    },
    {
      "epoch": 3.9569313593539706,
      "grad_norm": 5.920103073120117,
      "learning_rate": 1.0430686406460296e-05,
      "loss": 0.7256,
      "step": 14700
    },
    {
      "epoch": 3.970390309555855,
      "grad_norm": 4.456234931945801,
      "learning_rate": 1.0296096904441454e-05,
      "loss": 0.7114,
      "step": 14750
    },
    {
      "epoch": 3.983849259757739,
      "grad_norm": 5.3292741775512695,
      "learning_rate": 1.016150740242261e-05,
      "loss": 0.678,
      "step": 14800
    },
    {
      "epoch": 3.9973082099596233,
      "grad_norm": 5.479163646697998,
      "learning_rate": 1.0026917900403769e-05,
      "loss": 0.7132,
      "step": 14850
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.6910755148741419,
      "eval_loss": 0.7530277371406555,
      "eval_runtime": 155.6732,
      "eval_samples_per_second": 33.686,
      "eval_steps_per_second": 4.214,
      "step": 14860
    },
    {
      "epoch": 4.010767160161508,
      "grad_norm": 7.5032958984375,
      "learning_rate": 9.892328398384927e-06,
      "loss": 0.6998,
      "step": 14900
    },
    {
      "epoch": 4.024226110363392,
      "grad_norm": 7.154052734375,
      "learning_rate": 9.757738896366085e-06,
      "loss": 0.69,
      "step": 14950
    },
    {
      "epoch": 4.037685060565276,
      "grad_norm": 7.912858963012695,
      "learning_rate": 9.623149394347242e-06,
      "loss": 0.8103,
      "step": 15000
    },
    {
      "epoch": 4.05114401076716,
      "grad_norm": 4.221042156219482,
      "learning_rate": 9.488559892328399e-06,
      "loss": 0.6373,
      "step": 15050
    },
    {
      "epoch": 4.064602960969045,
      "grad_norm": 8.972135543823242,
      "learning_rate": 9.353970390309557e-06,
      "loss": 0.7293,
      "step": 15100
    },
    {
      "epoch": 4.078061911170929,
      "grad_norm": 5.229021072387695,
      "learning_rate": 9.219380888290715e-06,
      "loss": 0.7096,
      "step": 15150
    },
    {
      "epoch": 4.091520861372813,
      "grad_norm": 4.287521839141846,
      "learning_rate": 9.084791386271871e-06,
      "loss": 0.7111,
      "step": 15200
    },
    {
      "epoch": 4.104979811574697,
      "grad_norm": 4.859043121337891,
      "learning_rate": 8.950201884253029e-06,
      "loss": 0.7665,
      "step": 15250
    },
    {
      "epoch": 4.118438761776582,
      "grad_norm": 6.399191856384277,
      "learning_rate": 8.815612382234187e-06,
      "loss": 0.6613,
      "step": 15300
    },
    {
      "epoch": 4.131897711978466,
      "grad_norm": 6.323063373565674,
      "learning_rate": 8.681022880215343e-06,
      "loss": 0.7346,
      "step": 15350
    },
    {
      "epoch": 4.14535666218035,
      "grad_norm": 6.499590873718262,
      "learning_rate": 8.546433378196501e-06,
      "loss": 0.8228,
      "step": 15400
    },
    {
      "epoch": 4.158815612382234,
      "grad_norm": 4.934663772583008,
      "learning_rate": 8.411843876177658e-06,
      "loss": 0.7695,
      "step": 15450
    },
    {
      "epoch": 4.172274562584119,
      "grad_norm": 6.80680513381958,
      "learning_rate": 8.277254374158816e-06,
      "loss": 0.7241,
      "step": 15500
    },
    {
      "epoch": 4.185733512786003,
      "grad_norm": 5.572939395904541,
      "learning_rate": 8.142664872139973e-06,
      "loss": 0.7564,
      "step": 15550
    },
    {
      "epoch": 4.199192462987887,
      "grad_norm": 10.559480667114258,
      "learning_rate": 8.00807537012113e-06,
      "loss": 0.7573,
      "step": 15600
    },
    {
      "epoch": 4.212651413189771,
      "grad_norm": 8.998347282409668,
      "learning_rate": 7.873485868102288e-06,
      "loss": 0.81,
      "step": 15650
    },
    {
      "epoch": 4.226110363391656,
      "grad_norm": 8.611777305603027,
      "learning_rate": 7.738896366083446e-06,
      "loss": 0.7499,
      "step": 15700
    },
    {
      "epoch": 4.23956931359354,
      "grad_norm": 9.317258834838867,
      "learning_rate": 7.604306864064603e-06,
      "loss": 0.7478,
      "step": 15750
    },
    {
      "epoch": 4.253028263795424,
      "grad_norm": 7.081197738647461,
      "learning_rate": 7.469717362045761e-06,
      "loss": 0.7273,
      "step": 15800
    },
    {
      "epoch": 4.2664872139973085,
      "grad_norm": 5.661534786224365,
      "learning_rate": 7.335127860026918e-06,
      "loss": 0.6988,
      "step": 15850
    },
    {
      "epoch": 4.279946164199193,
      "grad_norm": 7.174445629119873,
      "learning_rate": 7.200538358008076e-06,
      "loss": 0.7296,
      "step": 15900
    },
    {
      "epoch": 4.293405114401077,
      "grad_norm": 6.857833385467529,
      "learning_rate": 7.065948855989234e-06,
      "loss": 0.652,
      "step": 15950
    },
    {
      "epoch": 4.306864064602961,
      "grad_norm": 6.221195697784424,
      "learning_rate": 6.93135935397039e-06,
      "loss": 0.7898,
      "step": 16000
    },
    {
      "epoch": 4.3203230148048455,
      "grad_norm": 4.423523902893066,
      "learning_rate": 6.796769851951548e-06,
      "loss": 0.6964,
      "step": 16050
    },
    {
      "epoch": 4.33378196500673,
      "grad_norm": 9.00755500793457,
      "learning_rate": 6.662180349932706e-06,
      "loss": 0.7747,
      "step": 16100
    },
    {
      "epoch": 4.347240915208614,
      "grad_norm": 4.155436038970947,
      "learning_rate": 6.5275908479138625e-06,
      "loss": 0.7483,
      "step": 16150
    },
    {
      "epoch": 4.360699865410498,
      "grad_norm": 6.647116184234619,
      "learning_rate": 6.3930013458950205e-06,
      "loss": 0.6565,
      "step": 16200
    },
    {
      "epoch": 4.3741588156123825,
      "grad_norm": 2.6975369453430176,
      "learning_rate": 6.258411843876178e-06,
      "loss": 0.6843,
      "step": 16250
    },
    {
      "epoch": 4.387617765814267,
      "grad_norm": 10.155699729919434,
      "learning_rate": 6.123822341857336e-06,
      "loss": 0.6381,
      "step": 16300
    },
    {
      "epoch": 4.401076716016151,
      "grad_norm": 5.582287311553955,
      "learning_rate": 5.989232839838493e-06,
      "loss": 0.6347,
      "step": 16350
    },
    {
      "epoch": 4.414535666218035,
      "grad_norm": 6.182757377624512,
      "learning_rate": 5.854643337819651e-06,
      "loss": 0.7287,
      "step": 16400
    },
    {
      "epoch": 4.4279946164199195,
      "grad_norm": 5.909531593322754,
      "learning_rate": 5.720053835800808e-06,
      "loss": 0.6533,
      "step": 16450
    },
    {
      "epoch": 4.441453566621804,
      "grad_norm": 6.225383758544922,
      "learning_rate": 5.585464333781965e-06,
      "loss": 0.7542,
      "step": 16500
    },
    {
      "epoch": 4.454912516823688,
      "grad_norm": 8.287832260131836,
      "learning_rate": 5.450874831763122e-06,
      "loss": 0.6761,
      "step": 16550
    },
    {
      "epoch": 4.468371467025572,
      "grad_norm": 5.961180210113525,
      "learning_rate": 5.31628532974428e-06,
      "loss": 0.7327,
      "step": 16600
    },
    {
      "epoch": 4.481830417227457,
      "grad_norm": 9.197139739990234,
      "learning_rate": 5.181695827725438e-06,
      "loss": 0.7407,
      "step": 16650
    },
    {
      "epoch": 4.495289367429341,
      "grad_norm": 5.718029022216797,
      "learning_rate": 5.047106325706595e-06,
      "loss": 0.6715,
      "step": 16700
    },
    {
      "epoch": 4.508748317631225,
      "grad_norm": 6.270398139953613,
      "learning_rate": 4.912516823687752e-06,
      "loss": 0.7999,
      "step": 16750
    },
    {
      "epoch": 4.522207267833109,
      "grad_norm": 6.050699710845947,
      "learning_rate": 4.77792732166891e-06,
      "loss": 0.7257,
      "step": 16800
    },
    {
      "epoch": 4.535666218034994,
      "grad_norm": 8.672115325927734,
      "learning_rate": 4.6433378196500674e-06,
      "loss": 0.736,
      "step": 16850
    },
    {
      "epoch": 4.549125168236878,
      "grad_norm": 4.201326370239258,
      "learning_rate": 4.508748317631225e-06,
      "loss": 0.8334,
      "step": 16900
    },
    {
      "epoch": 4.562584118438762,
      "grad_norm": 8.606371879577637,
      "learning_rate": 4.3741588156123826e-06,
      "loss": 0.7837,
      "step": 16950
    },
    {
      "epoch": 4.576043068640646,
      "grad_norm": 8.661980628967285,
      "learning_rate": 4.2395693135935405e-06,
      "loss": 0.7207,
      "step": 17000
    },
    {
      "epoch": 4.589502018842531,
      "grad_norm": 3.493622064590454,
      "learning_rate": 4.104979811574698e-06,
      "loss": 0.7382,
      "step": 17050
    },
    {
      "epoch": 4.602960969044415,
      "grad_norm": 6.484125137329102,
      "learning_rate": 3.970390309555855e-06,
      "loss": 0.7106,
      "step": 17100
    },
    {
      "epoch": 4.616419919246299,
      "grad_norm": 7.025972366333008,
      "learning_rate": 3.835800807537012e-06,
      "loss": 0.7361,
      "step": 17150
    },
    {
      "epoch": 4.629878869448183,
      "grad_norm": 6.6930646896362305,
      "learning_rate": 3.70121130551817e-06,
      "loss": 0.7657,
      "step": 17200
    },
    {
      "epoch": 4.643337819650068,
      "grad_norm": 5.235955715179443,
      "learning_rate": 3.5666218034993275e-06,
      "loss": 0.6265,
      "step": 17250
    },
    {
      "epoch": 4.656796769851952,
      "grad_norm": 5.56927490234375,
      "learning_rate": 3.4320323014804846e-06,
      "loss": 0.7734,
      "step": 17300
    },
    {
      "epoch": 4.670255720053836,
      "grad_norm": 6.50403356552124,
      "learning_rate": 3.2974427994616418e-06,
      "loss": 0.7301,
      "step": 17350
    },
    {
      "epoch": 4.68371467025572,
      "grad_norm": 7.034473419189453,
      "learning_rate": 3.1628532974427997e-06,
      "loss": 0.7739,
      "step": 17400
    },
    {
      "epoch": 4.697173620457605,
      "grad_norm": 5.844073295593262,
      "learning_rate": 3.0282637954239573e-06,
      "loss": 0.8111,
      "step": 17450
    },
    {
      "epoch": 4.710632570659489,
      "grad_norm": 7.095224380493164,
      "learning_rate": 2.8936742934051144e-06,
      "loss": 0.7372,
      "step": 17500
    },
    {
      "epoch": 4.724091520861373,
      "grad_norm": 5.616539478302002,
      "learning_rate": 2.759084791386272e-06,
      "loss": 0.7436,
      "step": 17550
    },
    {
      "epoch": 4.737550471063257,
      "grad_norm": 11.706494331359863,
      "learning_rate": 2.6244952893674295e-06,
      "loss": 0.7519,
      "step": 17600
    },
    {
      "epoch": 4.751009421265142,
      "grad_norm": 6.237199306488037,
      "learning_rate": 2.489905787348587e-06,
      "loss": 0.7356,
      "step": 17650
    },
    {
      "epoch": 4.764468371467026,
      "grad_norm": 5.628299236297607,
      "learning_rate": 2.3553162853297442e-06,
      "loss": 0.7297,
      "step": 17700
    },
    {
      "epoch": 4.77792732166891,
      "grad_norm": 1.0816258192062378,
      "learning_rate": 2.2207267833109018e-06,
      "loss": 0.727,
      "step": 17750
    },
    {
      "epoch": 4.7913862718707945,
      "grad_norm": 4.134703159332275,
      "learning_rate": 2.0861372812920593e-06,
      "loss": 0.6665,
      "step": 17800
    },
    {
      "epoch": 4.804845222072679,
      "grad_norm": 5.228076934814453,
      "learning_rate": 1.951547779273217e-06,
      "loss": 0.6964,
      "step": 17850
    },
    {
      "epoch": 4.818304172274562,
      "grad_norm": 6.246973037719727,
      "learning_rate": 1.8169582772543742e-06,
      "loss": 0.7261,
      "step": 17900
    },
    {
      "epoch": 4.831763122476447,
      "grad_norm": 8.964518547058105,
      "learning_rate": 1.6823687752355318e-06,
      "loss": 0.7369,
      "step": 17950
    },
    {
      "epoch": 4.845222072678331,
      "grad_norm": 5.321885585784912,
      "learning_rate": 1.5477792732166891e-06,
      "loss": 0.7055,
      "step": 18000
    },
    {
      "epoch": 4.858681022880216,
      "grad_norm": 4.324098587036133,
      "learning_rate": 1.4131897711978465e-06,
      "loss": 0.762,
      "step": 18050
    },
    {
      "epoch": 4.872139973082099,
      "grad_norm": 6.837947845458984,
      "learning_rate": 1.278600269179004e-06,
      "loss": 0.6995,
      "step": 18100
    },
    {
      "epoch": 4.885598923283984,
      "grad_norm": 15.199623107910156,
      "learning_rate": 1.1440107671601614e-06,
      "loss": 0.7252,
      "step": 18150
    },
    {
      "epoch": 4.899057873485868,
      "grad_norm": 4.95823335647583,
      "learning_rate": 1.009421265141319e-06,
      "loss": 0.7327,
      "step": 18200
    },
    {
      "epoch": 4.912516823687753,
      "grad_norm": 6.624244689941406,
      "learning_rate": 8.748317631224764e-07,
      "loss": 0.6782,
      "step": 18250
    },
    {
      "epoch": 4.925975773889636,
      "grad_norm": 10.2579927444458,
      "learning_rate": 7.40242261103634e-07,
      "loss": 0.7673,
      "step": 18300
    },
    {
      "epoch": 4.939434724091521,
      "grad_norm": 5.358569145202637,
      "learning_rate": 6.056527590847914e-07,
      "loss": 0.7346,
      "step": 18350
    },
    {
      "epoch": 4.952893674293405,
      "grad_norm": 11.834003448486328,
      "learning_rate": 4.7106325706594887e-07,
      "loss": 0.6498,
      "step": 18400
    },
    {
      "epoch": 4.96635262449529,
      "grad_norm": 7.110663890838623,
      "learning_rate": 3.364737550471063e-07,
      "loss": 0.7533,
      "step": 18450
    },
    {
      "epoch": 4.979811574697173,
      "grad_norm": 8.454828262329102,
      "learning_rate": 2.018842530282638e-07,
      "loss": 0.7533,
      "step": 18500
    },
    {
      "epoch": 4.993270524899058,
      "grad_norm": 4.6144585609436035,
      "learning_rate": 6.729475100942127e-08,
      "loss": 0.6949,
      "step": 18550
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.6920289855072463,
      "eval_loss": 0.7511008381843567,
      "eval_runtime": 155.7605,
      "eval_samples_per_second": 33.667,
      "eval_steps_per_second": 4.212,
      "step": 18575
    }
  ],
  "logging_steps": 50,
  "max_steps": 18575,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.922711526989824e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
