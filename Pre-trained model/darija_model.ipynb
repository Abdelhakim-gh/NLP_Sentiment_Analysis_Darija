{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMFE6HCVE6J5gLZFAFliSIv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JLZbKDVdjaHp"},"outputs":[],"source":["pip install datasets evaluate"]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments, pipeline, AutoTokenizer, AutoModelForMaskedLM\n","from sklearn.model_selection import train_test_split\n","import torch\n","import pandas as pd\n","from datasets import Dataset"],"metadata":{"id":"u9ioZZZMkVGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# classification(comments)\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OC1vlUvfkSGY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP Project/NLP Dataset/sentences/darija _clean.csv')"],"metadata":{"id":"evLgpQnmkTXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1=df.copy()\n","df2=df.copy()"],"metadata":{"id":"S9slxN-6kYK2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Split the data into 80% train and 20% test\n","train_df, test_df = train_test_split(df1, test_size=0.2, random_state=42)\n","\n","# Step 2: Split the 80% training set into 65% train and 15% validation\n","train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n","\n","train_dataset = Dataset.from_pandas(train_df)\n","val_dataset = Dataset.from_pandas(val_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","print(f\"Training set size: {len(train_dataset)}\")\n","print(f\"Validation set size: {len(val_dataset)}\")\n","print(f\"Test set size: {len(test_dataset)}\")"],"metadata":{"id":"RDaRIVgdkaB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load DarijaBERT tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"SI2M-Lab/DarijaBERT\")\n","model = AutoModelForMaskedLM.from_pretrained(\"SI2M-Lab/DarijaBERT\")\n","\n","classification = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)"],"metadata":{"id":"MdWBWSOekb99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize function\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"cleaned_text\"], padding=\"max_length\", truncation=True, max_length=512)\n","\n","# Apply the tokenization to all splits (train, validation, and test)\n","train_dataset = train_dataset.map(tokenize_function, batched=True)\n","val_dataset = val_dataset.map(tokenize_function, batched=True)\n","test_dataset = test_dataset.map(tokenize_function, batched=True)"],"metadata":{"id":"ZHAIoYUWkejS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset"],"metadata":{"id":"W22LwWwYkhEu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_dataset"],"metadata":{"id":"sfqU02svkjfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset"],"metadata":{"id":"IRjYJEqRklNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1[\"label\"].value_counts()"],"metadata":{"id":"k7aucnVPknJq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Normalize labels to lowercase\n","train_dataset = train_dataset.map(lambda x: {'label': x['label'].lower()})\n","val_dataset = val_dataset.map(lambda x: {'label': x['label'].lower()})\n","test_dataset = test_dataset.map(lambda x: {'label': x['label'].lower()})\n","\n","label_map = {'negative': 0, 'neutral': 1, 'positive': 2, 'mixed': 3}\n","\n","# Apply label encoding\n","train_dataset = train_dataset.map(lambda x: {'labels': label_map[x['label']]})\n","val_dataset = val_dataset.map(lambda x: {'labels': label_map[x['label']]})\n","test_dataset = test_dataset.map(lambda x: {'labels': label_map[x['label']]})\n"],"metadata":{"id":"MYQjmz9DkpYI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","\n","# Load the pre-trained DarijaBERT model for classification\n","model = AutoModelForSequenceClassification.from_pretrained(\"SI2M-Lab/DarijaBERT\", num_labels=4)"],"metadata":{"id":"9eG3QxQRkr6i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Unique labels in the train dataset: {set(train_dataset['labels'])}\")\n","print(f\"Unique labels in the validation dataset: {set(val_dataset['labels'])}\")\n","print(f\"Unique labels in the test dataset: {set(test_dataset['labels'])}\")"],"metadata":{"id":"EKWhQqTlkuBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments, pipeline, AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","import torch\n","import pandas as pd\n","from datasets import Dataset\n","import numpy as np\n","import evaluate\n","import matplotlib.pyplot as plt\n","\n","# Load DarijaBERT tokenizer and model for sequence classification (not masked LM)\n","tokenizer = AutoTokenizer.from_pretrained(\"SI2M-Lab/DarijaBERT\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"SI2M-Lab/DarijaBERT\", num_labels=4)\n","\n","# Ensure the dataset is in the right format\n","train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","# Define compute_metrics function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=-1)  # Get the predicted class index\n","    accuracy_metric = evaluate.load(\"accuracy\")\n","    precision_metric = evaluate.load(\"precision\")\n","    recall_metric = evaluate.load(\"recall\")\n","    f1_metric = evaluate.load(\"f1\")\n","\n","    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n","    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n","\n","    return {\n","        \"accuracy\": accuracy[\"accuracy\"],\n","        \"precision\": precision[\"precision\"],\n","        \"recall\": recall[\"recall\"],\n","        \"f1\": f1[\"f1\"],\n","    }\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./darija_model',\n","    num_train_epochs=10,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=32,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=50,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    report_to=\"none\",  # Disable WandB logging\n","    dataloader_num_workers=4,\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train the model\n","train_output = trainer.train()\n","\n","# Evaluate the model\n","eval_results = trainer.evaluate()"],"metadata":{"id":"P0rn9bSPkzjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for log in log_history[:5]:  # Afficher les 5 premiers logs pour exemple\n","    print(log)"],"metadata":{"id":"hiqTmb_3lebZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n","train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n","eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n","eval_epochs = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n","\n","# Vérifiez les tailles\n","print(\"Epochs:\", len(epochs), \"Train Losses:\", len(train_losses))\n","print(\"Eval Epochs:\", len(eval_epochs), \"Eval Losses:\", len(eval_losses))\n","\n","# Plot Training vs Validation Loss\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, train_losses, label=\"Training Loss\", marker=\"o\")\n","plt.plot(eval_epochs, eval_losses, label=\"Validation Loss\", marker=\"o\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Training vs Validation Loss\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","# Si aucune accuracy n'est présente, informer l'utilisateur\n","if not eval_losses:\n","    print(\"Aucune métrique d'accuracy disponible à tracer.\")"],"metadata":{"id":"iegZ_blHlfZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","eval_accuracies = [log[\"eval_accuracy\"] for log in log_history if \"eval_accuracy\" in log]\n","eval_epochs_accuracy = [log[\"epoch\"] for log in log_history if \"eval_accuracy\" in log]\n","\n","if eval_accuracies:\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(eval_epochs_accuracy, eval_accuracies, label=\"Validation Accuracy\", marker=\"o\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.title(\"Validation Accuracy\")\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","else:\n","    print(\"Aucune métrique de validation d'accuracy à tracer.\")"],"metadata":{"id":"CMBRW-vGli8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test dataset\n","results = trainer.evaluate(test_dataset)\n","\n","# Print the results\n","print(f\"Test results: {results}\")"],"metadata":{"id":"FkjLJy5FloyO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the trained model\n","save_path = \"/content/drive/MyDrive/Colab Notebooks/NLP Project/SaveModel\"\n","\n","# Save the trained model and tokenizer\n","model.save_pretrained(save_path)\n","tokenizer.save_pretrained(save_path)\n","\n","print(f\"Model and tokenizer saved to {save_path}\")\n"],"metadata":{"id":"nuURX69glsSY"},"execution_count":null,"outputs":[]}]}