{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Training / Fine tuning\n","\n","> Execution :  Dependencies → Label Mapping\n","\n","## Before training\n","\n","### **Splitting the Data**\n","\n","```python\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset\n","\n","# Assuming `df1` is your initial dataset\n","# Split into 80% training and 20% testing\n","train_df, test_df = train_test_split(df1, test_size=0.2, random_state=42)\n","\n","# Further split the training set into 65% training and 15% validation\n","train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n","\n","# Create Hugging Face Datasets\n","train_dataset = Dataset.from_pandas(train_df)\n","val_dataset = Dataset.from_pandas(val_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","print(f\"Training set size: {len(train_dataset)}\")\n","print(f\"Validation set size: {len(val_dataset)}\")\n","print(f\"Test set size: {len(test_dataset)}\")\n","```\n","\n","### Tokenization\n","\n","```python\n","from transformers import AutoTokenizer\n","\n","# Load the tokenizer for DarijaBERT\n","tokenizer = AutoTokenizer.from_pretrained(\"SI2M-Lab/DarijaBERT\")\n","\n","# Define the tokenization function\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"cleaned_text\"], padding=\"max_length\", truncation=True, max_length=512)\n","\n","# Apply tokenization to all datasets\n","train_dataset = train_dataset.map(tokenize_function, batched=True)\n","val_dataset = val_dataset.map(tokenize_function, batched=True)\n","test_dataset = test_dataset.map(tokenize_function, batched=True)\n","```\n","\n","### Normalizing and Mapping Labels\n","\n","```python\n","# Normalize labels\n","def normalize_label(label):\n","    label_mapping = {\n","        'negative': 'negative',\n","        'Negative': 'negative',\n","        'négatif': 'negative',\n","        'neutral': 'neutral',\n","        'Neutral': 'neutral',\n","        'positif': 'positive',\n","        'Positive': 'positive',\n","        'positive': 'positive',\n","        'mixed': 'mixed',\n","        'Mixed': 'mixed',\n","        'neutre': 'neutral'\n","    }\n","    return label_mapping.get(label.lower(), label)\n","\n","# Map labels to numeric values\n","label_map = {'negative': 0, 'neutral': 1, 'positive': 2, 'mixed': 3}\n","\n","# Apply normalization and mapping\n","train_dataset = train_dataset.map(lambda x: {'label': normalize_label(x['label'])})\n","val_dataset = val_dataset.map(lambda x: {'label': normalize_label(x['label'])})\n","test_dataset = test_dataset.map(lambda x: {'label': normalize_label(x['label'])})\n","\n","train_dataset = train_dataset.map(lambda x: {'labels': label_map[x['label']]})\n","val_dataset = val_dataset.map(lambda x: {'labels': label_map[x['label']]})\n","test_dataset = test_dataset.map(lambda x: {'labels': label_map[x['label']]})\n","```\n","\n","### Formatting Datasets\n","\n","```python\n","# Set format for PyTorch tensors\n","train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","```\n","\n","**Data Preparation**:\n","\n","- Split into `train`, `val`, `test`.\n","- Tokenized the text and normalized/mapped labels.\n","\n","## Native\n","\n","```python\n","from transformers import Trainer, TrainingArguments, pipeline, AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","import torch\n","import pandas as pd\n","from datasets import Dataset\n","import numpy as np\n","import evaluate\n","import matplotlib.pyplot as plt\n","\n","# Load DarijaBERT tokenizer and model for sequence classification (not masked LM)\n","tokenizer = AutoTokenizer.from_pretrained(\"SI2M-Lab/DarijaBERT\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"SI2M-Lab/DarijaBERT\", num_labels=4)\n","\n","# Ensure the dataset is in the right format\n","train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","# Define compute_metrics function\n","# Define compute_metrics function with zero_division parameter\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=-1)  # Get the predicted class index\n","\n","    # Load metrics\n","    accuracy_metric = evaluate.load(\"accuracy\")\n","    precision_metric = evaluate.load(\"precision\")\n","    recall_metric = evaluate.load(\"recall\")\n","    f1_metric = evaluate.load(\"f1\")\n","\n","    # Compute metrics with zero_division handling\n","    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\", zero_division=1)\n","    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\", zero_division=1)\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\", zero_division=1)\n","\n","    # Return metrics\n","    return {\n","        \"accuracy\": accuracy[\"accuracy\"],\n","        \"precision\": precision[\"precision\"],\n","        \"recall\": recall[\"recall\"],\n","        \"f1\": f1[\"f1\"],\n","    }\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./model',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    report_to=\"none\",  # Disable WandB logging\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train the model\n","train_output = trainer.train()\n","\n","# Evaluate the model\n","eval_results = trainer.evaluate()\n","\n","```\n","\n","## PEFT / `LoRA`\n","\n","### v1\n","\n","```python\n","\tfrom transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n","\tfrom peft import LoraConfig, get_peft_model, TaskType\n","\tfrom sklearn.model_selection import train_test_split\n","\timport torch\n","\timport pandas as pd\n","\tfrom datasets import Dataset\n","\timport numpy as np\n","\timport evaluate\n","\timport matplotlib.pyplot as plt\n","\n","\t# Load DarijaBERT tokenizer and model for sequence classification\n","\ttokenizer = AutoTokenizer.from_pretrained(\"SI2M-Lab/DarijaBERT\")\n","\tmodel = AutoModelForSequenceClassification.from_pretrained(\"SI2M-Lab/DarijaBERT\", num_labels=4)\n","\n","\t# Configure LoRA\n","\tfrom peft import LoraConfig, get_peft_model\n","\n","\tlora_config = LoraConfig(\n","\t    task_type=TaskType.SEQ_CLS,  # Sequence Classification\n","\t    inference_mode=False,\n","\t    r=8,  # Low-rank dimension\n","\t    lora_alpha=32,  # Scaling factor\n","\t    lora_dropout=0.1  # Dropout probability\n","\t)\n","\n","\t# Wrap the model with LoRA\n","\tmodel = get_peft_model(model, lora_config)\n","\tprint(\"LoRA Model Ready\")\n","\n","\t# Ensure the dataset is in the right format\n","\ttrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\tval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","\n","\t# Define compute_metrics function\n","\tdef compute_metrics(eval_pred):\n","\t    predictions, labels = eval_pred\n","\t    predictions = np.argmax(predictions, axis=-1)  # Get the predicted class index\n","\n","\t    # Load metrics\n","\t    accuracy_metric = evaluate.load(\"accuracy\")\n","\t    precision_metric = evaluate.load(\"precision\")\n","\t    recall_metric = evaluate.load(\"recall\")\n","\t    f1_metric = evaluate.load(\"f1\")\n","\n","\t    # Compute metrics with zero_division handling\n","\t    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","\t    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\", zero_division=1)\n","\t    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\", zero_division=1)\n","\t    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\", zero_division=1)\n","\n","\t    return {\n","\t        \"accuracy\": accuracy[\"accuracy\"],\n","\t        \"precision\": precision[\"precision\"],\n","\t        \"recall\": recall[\"recall\"],\n","\t        \"f1\": f1[\"f1\"],\n","\t    }\n","\n","\t# Define training arguments\n","\ttraining_args = TrainingArguments(\n","\t    output_dir='./model',\n","\t    num_train_epochs=2,  # Set to 2 epochs for testing, adjust as needed\n","\t    per_device_train_batch_size=8,\n","\t    per_device_eval_batch_size=16,\n","\t    warmup_steps=500,\n","\t    weight_decay=0.01,\n","\t    logging_dir='./logs',\n","\t    logging_steps=10,\n","\t    eval_strategy=\"epoch\",\n","\t    save_strategy=\"epoch\",\n","\t    load_best_model_at_end=True,\n","\t    metric_for_best_model=\"accuracy\",\n","\t    report_to=\"none\",  # Disable WandB logging\n","\t)\n","\n","\t# Initialize the Trainer with LoRA-wrapped model\n","\ttrainer = Trainer(\n","\t    model=model,\n","\t    args=training_args,\n","\t    train_dataset=train_dataset,\n","\t    eval_dataset=val_dataset,\n","\t    tokenizer=tokenizer,\n","\t    compute_metrics=compute_metrics,\n","\t)\n","\n","\t# Train the model\n","\ttrain_output = trainer.train()\n","\n","\t# Evaluate the model\n","\teval_results = trainer.evaluate()\n","\n","\t# Save the LoRA fine-tuned model\n","\tmodel.save_pretrained(\"lora_fine_tuned_darijaBERT\")\n","\n","\t# Output results\n","\tprint(\"Training Complete\")\n","\tprint(\"Evaluation Results:\", eval_results)\n","```\n","\n","- **LoRA Configuration**: Configured with `r=8`, `lora_alpha=32`, and `lora_dropout=0.1` for efficient fine-tuning.\n","- **PEFT Library**: Integrated LoRA using the `peft` library and wrapped the `AutoModelForSequenceClassification` with LoRA adapters.\n","- **Checkpointing**: Save the LoRA fine-tuned model at the end for future use.\n","\n","### v2\n","\n","```python\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from peft import LoraConfig, get_peft_model, TaskType\n","import evaluate\n","import numpy as np\n","\n","# Load the model for sequence classification\n","model = AutoModelForSequenceClassification.from_pretrained(\"SI2M-Lab/DarijaBERT\", num_labels=4)\n","\n","# Configure LoRA for Sequence Classification\n","lora_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS,  # Sequence Classification\n","    inference_mode=False,\n","    r=8,  # Low-rank dimension\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n",")\n","\n","# Apply LoRA to the model\n","model = get_peft_model(model, lora_config)\n","\n","# Define metrics\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=-1)\n","    \n","    accuracy_metric = evaluate.load(\"accuracy\")\n","    f1_metric = evaluate.load(\"f1\")\n","    \n","    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n","    \n","    return {\n","        \"accuracy\": accuracy[\"accuracy\"],\n","        \"f1\": f1[\"f1\"]\n","    }\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./lora_finetuned_model\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_dir=\"./logs\",\n","    logging_steps=50,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    report_to=\"none\",  # Disable wandb\n",")\n","\n","# Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Save the fine-tuned model\n","model.save_pretrained(\"lora_finetuned_darijaBERT\")\n","\n","# Evaluate on test set\n","test_results = trainer.evaluate(test_dataset)\n","print(\"Test Results:\", test_results)\n","```\n","\n","- **LoRA Fine-Tuning**:\n","    - Applied LoRA to optimize memory and training time.\n","    - Fine-tuned the model using the labeled dataset.\n","- **Evaluation**: Evaluated the model on the test set after fine-tuning."],"metadata":{"id":"UJsRBdPFHqGm"}},{"cell_type":"code","source":[],"metadata":{"id":"zCXyUj32Kfbx"},"execution_count":null,"outputs":[]}]}